from sentence_transformers import SentenceTransformer, util
import nltk
from nltk.corpus import stopwords
import numpy as np

# Downloading stopwords for preprocessing
nltk.download('stopwords')
stop_words = set(stopwords.words('hindi'))  # You can change 'hindi' to other Indian languages

# Initialize the sentence transformer model
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

# Sample documents (You can replace these with your dataset)
documents = [
    "भारत एक महान देश है",
    "क्रिकेट भारत में बहुत लोकप्रिय है",
    "हम तकनीकी विकास की ओर अग्रसर हैं",
    "शिक्षा हर व्यक्ति का अधिकार है"
]

# Preprocess documents (basic example)
def preprocess(text):
    # Tokenize and remove stop words
    tokens = nltk.word_tokenize(text)
    filtered_tokens = [token for token in tokens if token not in stop_words]
    return ' '.join(filtered_tokens)
# Preprocessing the documents
processed_docs = [preprocess(doc) for doc in documents]

# Encode the documents to get their embeddings
doc_embeddings = model.encode(processed_docs)

# Function to perform semantic search
def search(query, top_k=2):
    query_embedding = model.encode(preprocess(query))
    # Calculate cosine similarities
    cos_scores = util.cos_sim(query_embedding, doc_embeddings)[0]
    # Get the top k most similar documents
    top_results = np.argsort(cos_scores)[::-1][:top_k]
    return [(documents[idx], cos_scores[idx]) for idx in top_results]

# Example search
query = "भारत में खेल"
results = search(query)
for result in results:
    print(f"Document: {result[0]}, Score: {result[1]:.4f}")